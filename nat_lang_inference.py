# -*- coding: utf-8 -*-
"""nat_lang_inference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAsdhqRgvi2hU7QAZFxvmCIwqdHjNHcy
"""

from sklearn.metrics import roc_curve, auc, RocCurveDisplay
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np
import pickle
from tqdm import tqdm
from nltk.corpus import stopwords
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
nltk.download('stopwords')

with open("mnli_pp.pkl", "rb") as f:
    mnli_dataset = pickle.load(f)
with open("mnli_word_to_index.pkl", "rb") as f:
    mnli_word2index = pickle.load(f)
with open("mnli_index_to_word.pkl", "rb") as f:
    mnli_index2word = pickle.load(f)
with open("mnli_word_to_vec.pkl", "rb") as f:
    mnli_embeddings = pickle.load(f)

class elmo(nn.Module):
    def __init__(self, vocab_size, embed_size, embeddings):
        super(elmo, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze = False)
        self.lstm1 = nn.LSTM(embed_size, embed_size//2, bidirectional=True)
        self.lstm2 = nn.LSTM(embed_size, embed_size//2, bidirectional=True)
        self.fc = nn.Linear(embed_size, vocab_size)

    def forward(self, x):
        out = self.embedding(x)
        out1, _ = self.lstm1(out)
        out2, _ = self.lstm2(out1)
        out_fc = self.fc(out2.view(-1,100))
        elmo = 0.1*out + 0.3*out2 + 0.6*out2
        return out_fc, elmo

class nli(nn.Module):
    def __init__(self, glove_dim, elmo_dim, hidden_size, glove_embeddings, no_classes):
        super(nli, self).__init__()
        self.glove = nn.Embedding.from_pretrained(glove_embeddings, freeze = False)
        self.bilstm = nn.LSTM(2*(glove_dim+elmo_dim), hidden_size, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, no_classes)
        self.sg = nn.Sigmoid()

    def forward(self, prem, hyp, prem_elmo_emb, hyp_elmo_emb):
        prem_glove_emb = self.glove(prem)
        hyp_glove_emb = self.glove(hyp)
        out_bilstm, _ = self.bilstm(torch.cat((prem_glove_emb, prem_elmo_emb, hyp_glove_emb, hyp_elmo_emb), dim=2))
        out_fc = self.fc(out_bilstm)
        out_sg = self.sg(out_fc)
        return out_sg

class mnlidataset(Dataset):
    def __init__(self, data, sent_length):
        # print('Building MNLIDataset...')
        padded_premise = []
        padded_hypothesis = []
        labels = []
        for prem, hyp, label in zip(data['premise_itokens'], data['hypothesis_itokens'], data['label']):
            prem+=[0]*sent_length
            hyp+=[0]*sent_length
            padded_premise.append(prem[:sent_length])
            padded_hypothesis.append(hyp[:sent_length])
            one_hot_label = [0,0,0]
            one_hot_label[label] = 1
            labels.append(one_hot_label)
        self.premise = torch.tensor(padded_premise)
        self.hypothesis = torch.tensor(padded_hypothesis)
        self.labels = torch.tensor(labels, dtype=torch.float)
        # print('Building MNLIDataset complete...')

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return (self.premise[index], self.hypothesis[index], self.labels[index])


sent_length = 20

train_dataset = mnlidataset(mnli_dataset['train'], sent_length)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

vocab_size = len(mnli_word2index)
glove_dim = 100
mnli_elmo = elmo(vocab_size, glove_dim, mnli_embeddings)
mnli_elmo.load_state_dict(torch.load("mnli_elmo_model.pt"))

elmo_dim = 100
hidden_size = 128
no_classes = 3
model = nli(glove_dim, elmo_dim, hidden_size, mnli_embeddings, no_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
num_epochs = 20
model_name = "nl_inference.pt"

#                                                                  TRAINING MODEL
# for epoch in range(num_epochs):
#     print(f'Epoch {epoch}:')
#     running_loss = 0.0
#     running_corrects = 0
#     for i, batch in enumerate(train_loader):
#         optimizer.zero_grad()
#         premise_input_tensor, hyp_input_tensor, target_tensor = batch
#         outputs = model(premise_input_tensor, hyp_input_tensor,mnli_elmo(premise_input_tensor)[1],mnli_elmo(hyp_input_tensor)[1])
#         outputs = torch.mean(outputs, dim=1)
#         loss = criterion(outputs, target_tensor)
#         loss.backward()
#         optimizer.step()

#         running_loss += loss.item()

#         preds = torch.argmax(outputs, dim=1)
#         labels = torch.argmax(target_tensor, dim=1)
#         running_corrects += torch.sum(preds == labels).item()

#     train_loss = running_loss / len(train_loader)
#     epoch_acc = running_corrects / len(train_loader)
#     print(f'Training loss for epoch {epoch + 1}: {train_loss:.4f}')
#     print(f'Training accu for epoch {epoch + 1}: {epoch_acc :.4f}')
#     torch.save(model.state_dict(), model_name)



test_dataset = mnlidataset(mnli_dataset['test'], sent_length)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)



model.eval()
running_loss = 0.0
running_corrects = 0

#                                                                       TESTING MODEL
# with torch.no_grad():
#     for i, batch in enumerate(test_loader):
#         premise_input_tensor, hyp_input_tensor, target_tensor = batch
#         outputs = model(premise_input_tensor, hyp_input_tensor, mnli_elmo(premise_input_tensor)[1], mnli_elmo(hyp_input_tensor)[1])
#         outputs = torch.mean(outputs, dim=1)
#         loss = criterion(outputs, target_tensor)

#         running_loss += loss.item()

#         preds = torch.argmax(outputs, dim=1)
#         labels = torch.argmax(target_tensor, dim=1)
#         running_corrects += torch.sum(preds == labels).item()

# res_loss = running_loss / len(test_loader)
# res_acc = running_corrects / len(test_loader)

# print(f'Loss: {res_loss:.4f}')
# print(f'Accu: {res_acc :.4f}')

# return res_loss, res_acc

def generate_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    ax = sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('True labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(labels)
    ax.yaxis.set_ticklabels(labels)
    return ax

def plot_roc(y_true, y_prob, num_classes):
    y_true = torch.tensor(y_true)
    y_prob = torch.tensor(y_prob)
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true, y_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Compute micro-average ROC curve and ROC area
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true.ravel(), y_prob.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    # Plot ROC curve for each class
    fig, ax = plt.subplots(figsize=(8, 6))
    for i in range(num_classes):
        roc_display = RocCurveDisplay(
            fpr=fpr[i], tpr=tpr[i], roc_auc=roc_auc[i], estimator_name=f"Class {i}")
        roc_display.plot(ax=ax)

    # Plot micro-average ROC curve
    roc_display = RocCurveDisplay(fpr=fpr["micro"], tpr=tpr["micro"], roc_auc=roc_auc["micro"],
                                  estimator_name="Micro-average")
    roc_display.plot(ax=ax, linestyle="--")

    # Set axis labels and title
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("Receiver Operating Characteristic (ROC) Curves")
    ax.legend()
    plt.show()

