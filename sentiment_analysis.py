# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GIx14VTfcypU6QU0bn-5vBcGbD0nRUy9
"""


!pip install seaborn

from sklearn.metrics import roc_curve, auc, RocCurveDisplay
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import numpy as np
import pickle
from nltk.corpus import stopwords
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer
nltk.download('stopwords')

with open("sst_pp.pkl", "rb") as f:
    sst_dataset = pickle.load(f)
with open("sst_word_to_index.pkl", "rb") as f:
    word_to_index = pickle.load(f)
with open("sst_word_to_index.pkl", "rb") as f:
    index_to_word = pickle.load(f)
with open("sst_word_to_vec.pkl", "rb") as f:
    sst_embeddings = pickle.load(f)

class elmo(nn.Module):
    def __init__(self, vocab_size, embed_size, embeddings):
        super(elmo, self).__init__()
        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze = False)
        self.lstm1 = nn.LSTM(embed_size, embed_size//2, bidirectional=True)
        self.lstm2 = nn.LSTM(embed_size, embed_size//2, bidirectional=True)
        self.fc = nn.Linear(embed_size, vocab_size)

    def forward(self, x):
        out = self.embedding(x)
        out1, _ = self.lstm1(out)
        out2, _ = self.lstm2(out1)
        out_fc = self.fc(out2.view(-1,100))
        elmo_final = 0.1*out + 0.3*out2 + 0.6*out2
        return out_fc, elmo_final

class sentiment_classification(nn.Module):
    def __init__(self, glove_dim, elmo_dim, hidden_size, glove_embeddings, no_classes):
        super(sentiment_classification, self).__init__()
        self.glove = nn.Embedding.from_pretrained(glove_embeddings, freeze = False)
        self.bilstm = nn.LSTM(glove_dim+elmo_dim, hidden_size, bidirectional=True)
        self.fc = nn.Linear(hidden_size*2, no_classes)
        self.sg = nn.Sigmoid()

    def forward(self, inp, elmo_emb):
        glove_emb = self.glove(inp)
        out_bilstm, _ = self.bilstm(torch.cat((glove_emb, elmo_emb), dim=2))
        out_fc = self.fc(out_bilstm)
        out_sg = self.sg(out_fc)
        return out_sg

class SSTDataset(Dataset):
    def __init__(self, data, sent_length):
        # print('Building SSTDataset...')
        padded_data = []
        labels = []
        for text, label in zip(data['itokens'], data['label']):
            text+=[0]*sent_length
            padded_data.append(text[:sent_length])
            one_hot_label = [0,0]
            label = round(float(label))
            one_hot_label[label] = 1
            labels.append(one_hot_label)
        self.data = torch.tensor(padded_data)
        self.labels = torch.tensor(labels, dtype=torch.float)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return (self.data[index], self.labels[index])

sent_length = 20

train_dataset = SSTDataset(sst_dataset['train'], sent_length)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

vocab_size = len(word_to_index)
glove_dim = 100
sst_elmo = elmo(vocab_size, glove_dim, sst_embeddings)
sst_elmo.load_state_dict(torch.load("sst_elmo_model.pt"))

elmo_dim = 100
hidden_size = 256
no_classes = 2
sst_model = sentiment_classification(glove_dim, elmo_dim, hidden_size, sst_embeddings, no_classes)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(sst_model.parameters())
num_epochs = 15
model_name = "sentiment_classification_model.pt"

#                                                                        TRAINING
# for epoch in range(num_epochs):
#     running_loss = 0.0
#     running_corrects = 0
#     for i, batch in enumerate(train_loader):
#       optimizer.zero_grad()
#       input_tensor, target_tensor = batch
#       outputs = sst_model(input_tensor, sst_elmo(input_tensor)[1])
#       outputs = torch.mean(outputs, dim=1)
#       loss = criterion(outputs, target_tensor)
#       loss.backward()
#       optimizer.step()
#       running_loss += loss.item()
#       preds = torch.argmax(outputs, dim=1)
#       labels = torch.argmax(target_tensor, dim=1)
#       running_corrects += torch.sum(preds == labels).item()
#     train_loss = running_loss / len(train_loader)
#     epoch_acc = running_corrects / len(train_loader)
#     print(f'Training loss for epoch {epoch + 1}: {train_loss:.4f}')
#     print(f'Training accuracy for epoch {epoch + 1}: {epoch_acc :.4f}')
#     torch.save(sst_model.state_dict(), model_name)

test_dataset = SSTDataset(sst_dataset['test'], sent_length)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)

all_preds = []
all_preds_probs = []
all_actual = []

sst_model.eval()
running_loss = 0
running_corrects = 0


#                                                                       TESTING
# with torch.no_grad():
#     for i, batch in enumerate(test_loader):
#         input_tensor, target_tensor = batch
#         outputs = sst_model(input_tensor, sst_elmo(input_tensor)[1])
#         outputs = torch.mean(outputs, dim=1)
#         all_preds_probs += [[prob.item() for prob in output]for output in outputs]
#         loss = criterion(outputs, target_tensor)

#         running_loss += loss.item()

#         preds = torch.argmax(outputs, dim=1)
#         all_preds += [pred.item() for pred in preds]
#         labels = torch.argmax(target_tensor, dim=1)
#         all_actual += [label.item() for label in labels]
#         running_corrects += torch.sum(preds == labels).item()

res_loss = running_loss / len(test_loader)
res_acc = running_corrects / len(test_loader)

print(f'Loss: {res_loss:.4f}')
print(f'Accu: {res_acc :.4f}')

def generate_confusion_matrix(y_true, y_pred, labels):
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    ax = sns.heatmap(cm, annot=True, cmap='Blues', fmt='g')
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('True labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels(labels)
    ax.yaxis.set_ticklabels(labels)
    return ax

def plot_roc(y_true, y_prob, num_classes):
    y_true = torch.tensor(y_true)
    y_prob = torch.tensor(y_prob)
    # Compute ROC curve and ROC area for each class
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(num_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true, y_prob[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])

    # Plot ROC curve for each class
    fig, ax = plt.subplots(figsize=(8, 6))
    for i in range(num_classes):
        roc_display = RocCurveDisplay(
            fpr=fpr[i], tpr=tpr[i], roc_auc=roc_auc[i], estimator_name=f"Class {i}")
        roc_display.plot(ax=ax)

    # Set axis labels and title
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.set_title("Receiver Operating Characteristic (ROC) Curves")
    ax.legend()
    plt.show()

print(classification_report(all_actual, all_preds))
generate_confusion_matrix(all_actual, all_preds, list(range(no_classes)))
plot_roc(all_actual, all_preds_probs, no_classes)